{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "n = 3  # 3-gram\n",
    "\n",
    "data_path       = '../2_预处理/data/'          # 存放预处理后新闻数据的目录\n",
    "wordtable_path  = '../2_预处理/wordtable.txt'  # 词表路径\n",
    "stopwords_path  = '../2_预处理/stopwords/中文停用词表.txt' # 停止词表路径\n",
    "testset_path    = './testset/'       # 测试集的目录\n",
    "prediction_path = './predictions/'   # 存放预测结果的目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list = []  # n元组（分子）\n",
    "prefix_list = []  # n-1元组（分母）\n",
    "\n",
    "# 遍历所有预处理过的新闻文件\n",
    "for i, datafile in enumerate(os.listdir(data_path)):\n",
    "    with open(data_path + datafile, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sentence = ['<BOS>'] + line.split() + ['<EOS>']  # 列表，形如：['<BOS>', '显得', '十分', '明亮', '<EOS>']\n",
    "            ngrams = list(zip(*[sentence[i:] for i in range(n)]))   # 一个句子中n-gram元组的列表\n",
    "            prefix = list(zip(*[sentence[i:] for i in range(n-1)])) # 历史前缀元组的列表\n",
    "            ngrams_list += ngrams\n",
    "            prefix_list += prefix\n",
    "\n",
    "ngrams_counter = Counter(ngrams_list)\n",
    "prefix_counter = Counter(prefix_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []  # 词表中的全部词\n",
    "with open(wordtable_path, encoding='utf-8') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        all_words.append(line.split()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停止词\n",
    "with open(stopwords_path) as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = set(map(lambda x:x.strip(), stopwords))  # 去除末尾换行符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(sentence):\n",
    "    \"\"\"\n",
    "    计算一个句子的概率。\n",
    "    Params:\n",
    "        sentence: 由词构成的列表表示的句子。\n",
    "    Returns:\n",
    "        句子的概率。\n",
    "    \"\"\"\n",
    "    prob = 1  # 初始化句子概率\n",
    "    ngrams = list(zip(*[sentence[i:] for i in range(n)]))   # 将句子处理成n-gram的列表\n",
    "    for ngram in ngrams:\n",
    "        # 累乘每个n-gram的概率，并使用加一法进行数据平滑\n",
    "        prob *= (1 + ngrams_counter[ngram]) / (len(prefix_counter) + prefix_counter[(ngram[0], ngram[1])])\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pre_sentence, post_sentence, all_words, cand_num=1):\n",
    "    \"\"\"\n",
    "    根据历史进行一个词的预测。\n",
    "    Params:\n",
    "        pre_sentence: 待预测词之前部分句子的分词结果构成的列表。\n",
    "        post_sentence: 待预测词之后部分句子的分词结果构成的列表。\n",
    "        all_words: 所有候选词构成的列表。\n",
    "        cand_num: 候选词数，默认为1。\n",
    "    Returns:\n",
    "        一个含有cand_num个元素的列表，表示预测的词，概率由大到小排序；\n",
    "        如果预测失败，返回None。\n",
    "    \"\"\"\n",
    "    word_prob = []  # 候选词及其概率构成的元组的列表\n",
    "    for word in all_words:\n",
    "        # 实际上不需要算整个句子的概率，只需要算待预测词附近的概率即可，因为句子其他部分的概率不受待预测词影响\n",
    "        test_sentence = pre_sentence[-(n-1):] + [word] + post_sentence[:(n-1)]  # 待预测词及其前后各n-1个词的列表\n",
    "        word_prob.append( (word, probability(test_sentence)) )                  # (词, 概率)元组构成的列表\n",
    "\n",
    "    return sorted(word_prob, key=lambda tup: tup[1], reverse=True)[:cand_num]  # 按概率降序排序并取前cand_num个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/8l/s96gb4dx64sbmzrfs718s78r0000gn/T/jieba.cache\n",
      "Loading model cost 0.653 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 一直以来，有不少家长误认为，在开车时将孩子放在儿童安全 [座椅] 上是最为安全的方式。\n",
      "5 体验人员在驴妈妈平台体验预订景点门票时，发现 [故宫] 门票均绑定了内馆门票、导览、讲解等收费项目，没有单卖的故宫门票供选择，涉嫌强制消费。\n",
      "11 总而言之，从大环境上来说，目前仍然是处于教育用户的阶段，加上扫地 [机器人] 品类的下限很低，市场还没有形成一个足够清晰的认知。\n",
      "12 TOF技术虽然在手机应用上占据了大量的市场，但大多数 [应用] 由于比较“鸡肋”而难以支撑其进一步发展。\n",
      "14 在谈到与特斯拉的竞争时，小鹏汽车高管表示，小鹏没有照搬特斯拉的系统，但特斯拉系统中很好的部分会去 [学习] ，与此同时特斯拉踩过坑我们可以避免。\n",
      "18 在三大运营商积极展开5G部署工作的同时，各大手机 [厂商] 也在积极布局5G产品，说到这里就不能不提到OPPO了。\n",
      "29 学生禁带手机进校园的规定落实之初，被抓到玩手机的学生会由 [班主任] 谈话，没收的手机交给家长。\n",
      "36 随后李国庆微博发长文回应，称俞渝对我私生活做出的 [诽谤] 和诬蔑，我只想在这里回应一句话：等着收律师函吧。\n",
      "39 对大多数人来说，再花钱购买另一项订阅 [服务] 可能听起来并不是很有吸引力，但苹果押注其初创公司云集的内容，即使是最节俭的客户也会被说服。\n",
      "52 9月10日晚间消息，阿里20周年年会今日举行，马云登台发言正式 [宣布] 卸任阿里巴巴董事局主席。\n",
      "79 安信证券研报指出，随着资本市场对人工智能认知的不断深入，市场对 [人工智能] 的投资日趋成熟和理性，投融资频次在2018年以来有所放缓，但投资金额持续增加。\n",
      "81 上海移动的工作人员也向记者表示，暂时没有听说停止销售该类套餐。目前三大运营商中，只有中国电信明确将停售达量限速 [套餐] 。\n"
     ]
    }
   ],
   "source": [
    "# 加载测试集标签（答案）\n",
    "with open('testset/answer.txt', encoding='utf-8') as f:\n",
    "    answers = [answer.strip() for answer in f]  # 答案构成的列表\n",
    "    \n",
    "prediction_file = open(prediction_path + 'prediction_ngram.txt', 'w', encoding='utf-8')  # 存放预测结果\n",
    "\n",
    "# 开始测试\n",
    "correct_count = 0  # 预测正确的数量\n",
    "\n",
    "with open('testset/questions.txt', encoding='utf-8') as f:\n",
    "    questions = f.readlines()  # 测试集规模\n",
    "    total_count = len(questions)\n",
    "    for i, question in enumerate(questions):\n",
    "        question = question.strip()\n",
    "        pre_mask = question[:question.index('[MASK]')]     # 待预测词的历史\n",
    "        post_mask = question[question.index('[MASK]')+6:]  # 待预测词后的剩余部分\n",
    "        \n",
    "        pre_sentence = jieba.cut(pre_mask.replace('，', ' '))  # 分词\n",
    "        post_sentence = jieba.cut(post_mask.replace('，', ' '))  # 分词\n",
    "        pre_sentence = [word.strip() for word in pre_sentence if word.strip() and word not in stopwords]  # 去除停止词、空串\n",
    "        post_sentence = [word.strip() for word in post_sentence if word.strip() and word not in stopwords]  # 去除停止词、空串\n",
    "\n",
    "        predict_cand = predict(pre_sentence, post_sentence, all_words)  # 预测一个概率最大的词\n",
    "        prediction_file.write(' '.join([w[0] for w in predict_cand]) + '\\n')  # 将预测结果写入文件\n",
    "\n",
    "        # 遍历多个预测结果\n",
    "        for j, p in enumerate(predict_cand):\n",
    "            if p[0] == answers[i]:\n",
    "                print(i, '{} [{}] {}'.format(pre_mask, p[0], post_mask))\n",
    "                correct_count += 1\n",
    "                break\n",
    "                    \n",
    "prediction_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率：12/100\n"
     ]
    }
   ],
   "source": [
    "print('准确率：{}/{}'.format(correct_count, total_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
